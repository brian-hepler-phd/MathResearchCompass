{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc34308",
   "metadata": {},
   "source": [
    "# Math Research Compass: Data Processing Workflow\n",
    "\n",
    "This notebook demonstrates the complete data processing workflow for the Math Research Compass project, from raw arXiv data to topic modeling and visualization.\n",
    "\n",
    "## 1. Data Collection and Filtering\n",
    "\n",
    "The project uses data from the [Kaggle ArXiv dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv), which is a mirror for approximately 2.7 million arXiv papers, and is updated weekly. This dataset consists of article metadata, specifically:\n",
    "* `id`: ArXiv ID (can be used to access the paper, see below)\n",
    "* `submitter`: Who submitted the paper\n",
    "* `authors`: Authors of the paper\n",
    "* `title`: Title of the paper\n",
    "* `comments`: Additional info, such as number of pages and figures\n",
    "* `journal-ref`: Information about the journal the paper was published in\n",
    "* `doi`: [Digital Object Identifier](https://www.doi.org)\n",
    "* `abstract`: The abstract of the paper\n",
    "* `categories`: Categories / tags in the ArXiv system\n",
    "* `versions`: A version history\n",
    "\n",
    "You can access each paper directly on ArXiv using these links:\n",
    "\n",
    "* `https://arxiv.org/abs/{id}`: Page for this paper including its abstract and further links\n",
    "* `https://arxiv.org/pdf/{id}`: Direct link to download the PDF\n",
    "\n",
    "\n",
    "We filter this dataset to focus only on mathematics papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4edfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10701bd70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brainhelper/miniforge3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 696603 mathematics papers\n",
      "Saved cleaned math dataset to data/cleaned/math_arxiv_snapshot.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Read the arXiv dataset\n",
    "df = pd.read_json('../data/raw/arxiv-metadata-oai-snapshot.json', lines=True)\n",
    "\n",
    "# Select only relevant columns\n",
    "cols = ['id', 'authors', 'title', 'categories', 'abstract', 'update_date', 'authors_parsed']\n",
    "df = df[cols]\n",
    "\n",
    "# Filter to only include math papers\n",
    "math_df = df[df['categories'].str.contains('math', na=False)]\n",
    "print(f\"Found {len(math_df)} mathematics papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741aa0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 696603 mathematics papers from the past 5 years\n",
      "Saved cleaned math dataset to data/cleaned/math_arxiv_snapshot.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Currently doesn't do anything, since 'year' extraction from `updated_date` datetime column hasn't been done yet. \n",
    "def filter_recent_years(df, years):\n",
    "    \"\"\"Filter dataframe to only include papers from the last N years.\"\"\"\n",
    "    if df.empty or \"year\" not in df.columns:\n",
    "        return df\n",
    "        \n",
    "    current_year = datetime.now().year\n",
    "    start_year = current_year - years\n",
    "    \n",
    "    filtered_df = df[df[\"year\"] >= start_year].copy()\n",
    "    print(f\"Filtered to {len(filtered_df)} papers from {start_year}-{current_year}\")\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "filtered_df = filter_recent_years(math_df,5)\n",
    "print(f\"Found {len(filtered_df)} mathematics papers from the past 5 years\")\n",
    "\n",
    "# Save the filtered dataset\n",
    "filepath = Path('data/cleaned/math_arxiv_snapshot.csv')\n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "filtered_df.to_csv(filepath)\n",
    "print(f\"Saved cleaned math dataset to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef0ae6",
   "metadata": {},
   "source": [
    "## 2. Topic Modeling with BERTopic\n",
    "\n",
    "The next step uses BERTopic to identify coherent research topics within the mathematics papers. BERTopic combines transformer-based embeddings with clustering algorithms to discover topics and their representative keywords.\n",
    "\n",
    "We run this analysis using the `topic_trends_analyzer.py` script, which:\n",
    "1. Processes the abstracts and titles of papers\n",
    "2. Uses Sentence-BERT to create embeddings\n",
    "3. Applies dimensionality reduction (UMAP)\n",
    "4. Performs clustering (HDBSCAN)\n",
    "5. Extracts representative keywords for each topic\n",
    "\n",
    "For the next step, the relevant file will be saved as `topic_info_{timestamp}.csv` in the `results/topics/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running topic_trends_analyzer.py, we can load the results:\n",
    "topic_df = pd.read_csv('results/topics/topic_info_20250509_193929.csv')\n",
    "print(f\"Discovered {len(topic_df[topic_df['Topic'] != -1])} topics (excluding outliers)\")\n",
    "\n",
    "# Display the first few topics\n",
    "topic_df.rename(columns=str.lower, inplace=True)\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0856c",
   "metadata": {},
   "source": [
    "\n",
    "The output of topic modeling includes:\n",
    "- Topic IDs (numerical labels)\n",
    "- Topics counts (number of papers in each topic)\n",
    "- Topic names (generated from keywords)\n",
    "- Representative keywords for each topic\n",
    "- Representative documents for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be085a",
   "metadata": {},
   "source": [
    "## 3. Merging Document-Topic Assignments\n",
    "\n",
    "Now we need to match each paper with its assigned topic from the topic modeling, from `document_topics_{timestamp}.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8dfbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document-topic assignments\n",
    "docs_df = pd.read_csv('results/topics/document_topics_20250509_221839.csv', low_memory=False)\n",
    "\n",
    "# Merge topic assignments with original paper metadata\n",
    "doc_topic_df = pd.merge(\n",
    "    math_df,\n",
    "    docs_df[['id', 'topic']],\n",
    "    on='id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Select and reorder columns\n",
    "columns = ['id', 'title', 'categories', 'abstract',\n",
    "           'update_date', 'authors_parsed', 'topic']\n",
    "doc_topic_df = doc_topic_df[columns]\n",
    "print(f\"Matched {len(doc_topic_df)} papers with their topic assignments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb6250",
   "metadata": {},
   "source": [
    "## 4. Enhancing Topic Labels with Claude\n",
    "\n",
    "The raw topic labels from BERTopic are informative but can be improved. We use Claude (Anthropic's LLM) to generate more descriptive and human-readable topic labels.\n",
    "\n",
    "For each topic, Claude is prompted with:\n",
    "- The list of top keywords for that topic\n",
    "- The mathematical subject areas\n",
    "- A request to generate both a concise and a detailed descriptive label\n",
    "\n",
    "Specifically, we use the prompt\n",
    "\n",
    "> You are a mathematician and data scientist specializing in interpreting topic modeling results.\n",
    ">    \n",
    ">I have a set of topics generated from a BERTopic model analyzing mathematical research papers from arXiv.\n",
    ">The papers are primarily from these mathematical subject areas: {', '.join(subjects)}.\n",
    ">    \n",
    ">For Topic {topic_id}, the top terms (with their weights) are:\n",
    ">{keywords_text}\n",
    ">    \n",
    ">Based on these keywords, please identify what mathematical research topic this represents.\n",
    ">Provide two labels:\n",
    ">1. A concise label (3-5 words max) that captures the essence of this topic\n",
    ">2. A more descriptive name that specifies the mathematical subfield (e.g., \"Algebraic Topology: Persistent Homology\")\n",
    ">    \n",
    ">Format your response like this:\n",
    ">\n",
    ">SHORT_LABEL: [Your concise label]\n",
    ">\n",
    ">DESCRIPTIVE_LABEL: [Your more descriptive label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6844a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the enhanced topic labels generated by topic_labeling.py\n",
    "topic_labels = pd.read_csv('results/topics/topic_info_20250509_221839_claude_labeled.csv')\n",
    "topic_labels.rename(columns=str.lower, inplace=True)\n",
    "topic_labels.rename(columns={\"shortlabel\": \"short_label\", \"descriptivelabel\": \"descriptive_label\"}, inplace=True)\n",
    "\n",
    "# Display sample of enhanced labels\n",
    "rel_cols = ['topic', 'short_label', 'descriptive_label']\n",
    "topic_labels[rel_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a43e98",
   "metadata": {},
   "source": [
    "## 5. Creating the Final Dataset\n",
    "\n",
    "With all components ready, we create the final dataset containing papers with their topic assignments and enhanced labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add descriptive labels to our document dataset\n",
    "papers_by_topic_df = pd.merge(\n",
    "    doc_topic_df,\n",
    "    topic_labels[rel_cols],\n",
    "    on=\"topic\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Remove outlier topics (labeled as -1)\n",
    "papers_by_topic_no_outliers = papers_by_topic_df[papers_by_topic_df['topic'] != -1]\n",
    "print(f\"Final dataset contains {len(papers_by_topic_no_outliers)} papers across {papers_by_topic_no_outliers['topic'].nunique()} topics\")\n",
    "\n",
    "# Save the final dataset\n",
    "papers_by_topic_no_outliers.to_parquet(\"results/topics/papers_by_topic_no_outliers.parquet\", index=False)\n",
    "print(\"Saved final dataset with topic labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c9fabb",
   "metadata": {},
   "source": [
    "## 6. Preparing Data for Visualization\n",
    "\n",
    "The last step prepares a summary dataset for the dashboard visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary dataset of topics for visualization\n",
    "plot_df = topic_labels[['topic', 'count', 'descriptive_label']]\n",
    "plot_df = plot_df[plot_df['topic'] != -1]  # Remove outlier topic\n",
    "\n",
    "# Save the summary dataset\n",
    "filepath = Path('results/topics/common_topics.csv')\n",
    "plot_df.to_csv(filepath, index=False)\n",
    "print(f\"Saved topic summary to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ef7ac",
   "metadata": {},
   "source": [
    "## 7. Category Distribution Analysis\n",
    "\n",
    "The final processing step calculates the distribution of arXiv categories within each topic and determines the primary category for each topic. This is handled by the `category_distribution.py` script, which:\n",
    "\n",
    "1. Reads the papers by topic (from `papers_by_topic_no_outliers.parquet`) and calculates the frequency of each arXiv category within each topic\n",
    "2. Creates a nested dictionary mapping topics to their category distributions\n",
    "3. Determines the primary category for each topic (most frequent category)\n",
    "4. Updates the common_topics.csv file with the primary_category column\n",
    "\n",
    "After running this script, the `common_topics.csv` file includes:\n",
    "- `topic`: numerical topic ID\n",
    "- `count`: number of papers in the topic\n",
    "- `descriptive_label`: human-readable topic description\n",
    "- `primary_category`: the most common arXiv category in that topic\n",
    "\n",
    "This enhanced dataset is then used by the Shiny dashboard for visualization and exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
